{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "Loading dataset from disk.\n",
      "(6264, 1024) (783, 1024) (784, 1024)\n",
      "(6264, 12) (783, 12) (784, 12)\n",
      "(6264, 12) (783, 12) (784, 12)\n"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "\n",
    "#Load bộ dữ liệu toxic21\n",
    "_, (train,valid,test), _ = dc.molnet.load_tox21()\n",
    "train_X, train_Y, train_w = train.X, train.y, train.w\n",
    "valid_X, valid_Y, valid_w = valid.X, valid.y, valid.w\n",
    "test_X, test_Y, test_w = test.X, test.y, test.w\n",
    "\n",
    "print(train_X.shape,valid_X.shape,test_X.shape) #in ra kích cỡ của vecto đặc trưng\n",
    "print(train_Y.shape,valid_Y.shape,test_Y.shape) #in ra kích cỡ của vecto nhãn\n",
    "print(train_w.shape,valid_w.shape,test_w.shape) #in ra kích cỡ của vecto trọng số mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6264, 1024) (783, 1024) (784, 1024)\n",
      "(6264,) (783,) (784,)\n",
      "(6264,) (783,) (784,)\n"
     ]
    }
   ],
   "source": [
    "#loại bỏ phần dữ liệu thừa\n",
    "train_Y = train_Y[:, 0]\n",
    "valid_Y = valid_Y[:, 0]\n",
    "test_Y = test_Y[:, 0]\n",
    "train_w = train_w[:, 0]\n",
    "valid_w = valid_w[:, 0]\n",
    "test_w = test_w[:, 0]\n",
    "\n",
    "print(train_X.shape,valid_X.shape,test_X.shape) #in ra kích cỡ của vecto đặc trưng\n",
    "print(train_Y.shape,valid_Y.shape,test_Y.shape) #in ra kích cỡ của vecto nhãn\n",
    "print(train_w.shape,valid_w.shape,test_w.shape) #in ra kích cỡ của vecto trọng số mẫu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 \n",
    "n_epochs = 10 \n",
    "batch_size = 50\n",
    "n_hidden = 50\n",
    "dropout_prob = 1\n",
    "d = train_X.shape[1] #số chiều của vecto đặc trưng = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "#khởi tạo ô trống với kích cỡ minibatch chấp nhận nhiều giá trị khác nhau để nhập vecto đặc trưng x và nhãn y \n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32,(None,d))\n",
    "    y = tf.placeholder(tf.float32,(None,))\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#mô tả một lớp ẩn trong mạng với các tham số\n",
    "with tf.name_scope(\"hidden-layer\"):\n",
    "    W = tf.Variable(tf.random_normal((d, n_hidden))) #n_hidden là số lớp mạng ẩn trong toàn bộ mạng\n",
    "    b = tf.Variable(tf.random_normal((n_hidden,)))\n",
    "    x_hidden = tf.nn.relu(tf.matmul(x,W) + b) # giá trị tại một nút trong mạng\n",
    "    \n",
    "    x_hidden = tf.nn.dropout(x_hidden,keep_prob) #sử dụng dropout \n",
    "\n",
    "#mô tả lớp đầu ra của mạng\n",
    "with tf.name_scope(\"output\"):\n",
    "    W = tf.Variable(tf.random_normal((n_hidden,1)))\n",
    "    b = tf.Variable(tf.random_normal((1,)))\n",
    "    y_logit = tf.matmul(x_hidden,W) + b\n",
    "    \n",
    "    y_one_prob = tf.sigmoid(y_logit) #dùng hàm sigmoid để đưa đầu ra về dạng xác suất\n",
    "\n",
    "    y_pred = tf.round(y_one_prob) #làm tròn kết quả để đưa ra dự đoán 0 hoặc 1\n",
    "\n",
    "#mô tả cách tính sự mất mát tại mỗi điểm dữ liệu\n",
    "with tf.name_scope(\"loss\"):\n",
    "    y_expand = tf.expand_dims(y,1) #thêm một chiều vào vecto y\n",
    "    #tìm cross entropy giữa mỗi điểm dữ liệu\n",
    "    entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_logit,labels=y_expand) \n",
    "    #tính tổng các entropy \n",
    "    l = tf.reduce_sum(entropy)\n",
    "\n",
    "#mô tả hàm tối ưu cho mạng   \n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate).minimize(l)\n",
    "    \n",
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\",l)\n",
    "    merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_writer = tf.summary.FileWriter('/tmp/logistic-train', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, step: 0, loss: 359.763672 \n",
      "Epoch: 0, step: 1, loss: 459.091278 \n",
      "Epoch: 0, step: 2, loss: 570.972717 \n",
      "Epoch: 0, step: 3, loss: 434.572113 \n",
      "Epoch: 0, step: 4, loss: 385.434814 \n",
      "Epoch: 0, step: 5, loss: 360.103699 \n",
      "Epoch: 0, step: 6, loss: 384.246216 \n",
      "Epoch: 0, step: 7, loss: 422.724243 \n",
      "Epoch: 0, step: 8, loss: 513.125061 \n",
      "Epoch: 0, step: 9, loss: 268.475037 \n",
      "Epoch: 0, step: 10, loss: 361.075348 \n",
      "Epoch: 0, step: 11, loss: 534.608643 \n",
      "Epoch: 0, step: 12, loss: 414.832336 \n",
      "Epoch: 0, step: 13, loss: 412.949432 \n",
      "Epoch: 0, step: 14, loss: 226.162949 \n",
      "Epoch: 0, step: 15, loss: 296.734772 \n",
      "Epoch: 0, step: 16, loss: 310.046234 \n",
      "Epoch: 0, step: 17, loss: 185.366333 \n",
      "Epoch: 0, step: 18, loss: 258.668030 \n",
      "Epoch: 0, step: 19, loss: 230.514435 \n",
      "Epoch: 0, step: 20, loss: 165.606537 \n",
      "Epoch: 0, step: 21, loss: 281.208282 \n",
      "Epoch: 0, step: 22, loss: 257.746613 \n",
      "Epoch: 0, step: 23, loss: 249.002365 \n",
      "Epoch: 0, step: 24, loss: 281.886444 \n",
      "Epoch: 0, step: 25, loss: 181.461105 \n",
      "Epoch: 0, step: 26, loss: 383.020172 \n",
      "Epoch: 0, step: 27, loss: 328.930145 \n",
      "Epoch: 0, step: 28, loss: 253.472443 \n",
      "Epoch: 0, step: 29, loss: 293.755890 \n",
      "Epoch: 0, step: 30, loss: 250.726837 \n",
      "Epoch: 0, step: 31, loss: 260.644775 \n",
      "Epoch: 0, step: 32, loss: 170.431702 \n",
      "Epoch: 0, step: 33, loss: 253.220444 \n",
      "Epoch: 0, step: 34, loss: 199.822006 \n",
      "Epoch: 0, step: 35, loss: 130.075867 \n",
      "Epoch: 0, step: 36, loss: 259.221405 \n",
      "Epoch: 0, step: 37, loss: 241.992401 \n",
      "Epoch: 0, step: 38, loss: 184.250809 \n",
      "Epoch: 0, step: 39, loss: 72.949654 \n",
      "Epoch: 0, step: 40, loss: 234.706406 \n",
      "Epoch: 0, step: 41, loss: 153.258392 \n",
      "Epoch: 0, step: 42, loss: 200.892090 \n",
      "Epoch: 0, step: 43, loss: 311.798584 \n",
      "Epoch: 0, step: 44, loss: 212.503601 \n",
      "Epoch: 0, step: 45, loss: 262.953644 \n",
      "Epoch: 0, step: 46, loss: 172.016968 \n",
      "Epoch: 0, step: 47, loss: 143.342392 \n",
      "Epoch: 0, step: 48, loss: 259.743530 \n",
      "Epoch: 0, step: 49, loss: 127.929070 \n",
      "Epoch: 0, step: 50, loss: 146.202545 \n",
      "Epoch: 0, step: 51, loss: 51.731823 \n",
      "Epoch: 0, step: 52, loss: 113.654648 \n",
      "Epoch: 0, step: 53, loss: 109.365921 \n",
      "Epoch: 0, step: 54, loss: 225.733185 \n",
      "Epoch: 0, step: 55, loss: 312.713440 \n",
      "Epoch: 0, step: 56, loss: 328.058716 \n",
      "Epoch: 0, step: 57, loss: 204.657455 \n",
      "Epoch: 0, step: 58, loss: 120.937897 \n",
      "Epoch: 0, step: 59, loss: 75.414040 \n",
      "Epoch: 0, step: 60, loss: 96.662415 \n",
      "Epoch: 0, step: 61, loss: 183.723373 \n",
      "Epoch: 0, step: 62, loss: 108.946526 \n",
      "Epoch: 0, step: 63, loss: 147.245026 \n",
      "Epoch: 0, step: 64, loss: 102.155830 \n",
      "Epoch: 0, step: 65, loss: 81.770447 \n",
      "Epoch: 0, step: 66, loss: 143.772614 \n",
      "Epoch: 0, step: 67, loss: 108.080284 \n",
      "Epoch: 0, step: 68, loss: 195.114670 \n",
      "Epoch: 0, step: 69, loss: 190.217758 \n",
      "Epoch: 0, step: 70, loss: 122.903915 \n",
      "Epoch: 0, step: 71, loss: 142.514221 \n",
      "Epoch: 0, step: 72, loss: 53.314182 \n",
      "Epoch: 0, step: 73, loss: 176.105957 \n",
      "Epoch: 0, step: 74, loss: 83.030006 \n",
      "Epoch: 0, step: 75, loss: 205.236694 \n",
      "Epoch: 0, step: 76, loss: 165.051025 \n",
      "Epoch: 0, step: 77, loss: 67.135628 \n",
      "Epoch: 0, step: 78, loss: 148.164658 \n",
      "Epoch: 0, step: 79, loss: 121.286674 \n",
      "Epoch: 0, step: 80, loss: 63.295799 \n",
      "Epoch: 0, step: 81, loss: 131.671921 \n",
      "Epoch: 0, step: 82, loss: 179.507019 \n",
      "Epoch: 0, step: 83, loss: 69.307816 \n",
      "Epoch: 0, step: 84, loss: 179.689346 \n",
      "Epoch: 0, step: 85, loss: 23.132641 \n",
      "Epoch: 0, step: 86, loss: 99.898399 \n",
      "Epoch: 0, step: 87, loss: 122.347427 \n",
      "Epoch: 0, step: 88, loss: 115.443802 \n",
      "Epoch: 0, step: 89, loss: 86.487289 \n",
      "Epoch: 0, step: 90, loss: 94.354416 \n",
      "Epoch: 0, step: 91, loss: 63.872402 \n",
      "Epoch: 0, step: 92, loss: 31.628977 \n",
      "Epoch: 0, step: 93, loss: 120.536362 \n",
      "Epoch: 0, step: 94, loss: 45.732101 \n",
      "Epoch: 0, step: 95, loss: 168.067276 \n",
      "Epoch: 0, step: 96, loss: 164.175323 \n",
      "Epoch: 0, step: 97, loss: 29.874973 \n",
      "Epoch: 0, step: 98, loss: 54.584141 \n",
      "Epoch: 0, step: 99, loss: 39.675018 \n",
      "Epoch: 0, step: 100, loss: 39.616222 \n",
      "Epoch: 0, step: 101, loss: 116.381195 \n",
      "Epoch: 0, step: 102, loss: 60.756927 \n",
      "Epoch: 0, step: 103, loss: 111.002602 \n",
      "Epoch: 0, step: 104, loss: 106.016548 \n",
      "Epoch: 0, step: 105, loss: 59.427795 \n",
      "Epoch: 0, step: 106, loss: 96.095604 \n",
      "Epoch: 0, step: 107, loss: 36.827755 \n",
      "Epoch: 0, step: 108, loss: 30.455158 \n",
      "Epoch: 0, step: 109, loss: 108.742691 \n",
      "Epoch: 0, step: 110, loss: 39.938507 \n",
      "Epoch: 0, step: 111, loss: 77.182518 \n",
      "Epoch: 0, step: 112, loss: 17.108864 \n",
      "Epoch: 0, step: 113, loss: 75.408058 \n",
      "Epoch: 0, step: 114, loss: 67.400848 \n",
      "Epoch: 0, step: 115, loss: 66.552078 \n",
      "Epoch: 0, step: 116, loss: 128.389572 \n",
      "Epoch: 0, step: 117, loss: 113.283165 \n",
      "Epoch: 0, step: 118, loss: 80.029625 \n",
      "Epoch: 0, step: 119, loss: 30.650816 \n",
      "Epoch: 0, step: 120, loss: 95.465874 \n",
      "Epoch: 0, step: 121, loss: 120.763245 \n",
      "Epoch: 0, step: 122, loss: 98.018608 \n",
      "Epoch: 0, step: 123, loss: 165.422913 \n",
      "Epoch: 0, step: 124, loss: 120.001190 \n",
      "Epoch: 0, step: 125, loss: 25.033947 \n",
      "Epoch: 1, step: 126, loss: 41.969616 \n",
      "Epoch: 1, step: 127, loss: 67.405731 \n",
      "Epoch: 1, step: 128, loss: 50.550182 \n",
      "Epoch: 1, step: 129, loss: 53.481220 \n",
      "Epoch: 1, step: 130, loss: 88.779991 \n",
      "Epoch: 1, step: 131, loss: 18.130154 \n",
      "Epoch: 1, step: 132, loss: 126.893562 \n",
      "Epoch: 1, step: 133, loss: 185.045258 \n",
      "Epoch: 1, step: 134, loss: 166.839508 \n",
      "Epoch: 1, step: 135, loss: 96.301666 \n",
      "Epoch: 1, step: 136, loss: 132.598785 \n",
      "Epoch: 1, step: 137, loss: 164.346313 \n",
      "Epoch: 1, step: 138, loss: 137.079636 \n",
      "Epoch: 1, step: 139, loss: 73.845802 \n",
      "Epoch: 1, step: 140, loss: 10.405560 \n",
      "Epoch: 1, step: 141, loss: 47.116524 \n",
      "Epoch: 1, step: 142, loss: 66.146317 \n",
      "Epoch: 1, step: 143, loss: 43.519787 \n",
      "Epoch: 1, step: 144, loss: 60.097279 \n",
      "Epoch: 1, step: 145, loss: 46.079899 \n",
      "Epoch: 1, step: 146, loss: 65.010887 \n",
      "Epoch: 1, step: 147, loss: 43.513988 \n",
      "Epoch: 1, step: 148, loss: 106.500633 \n",
      "Epoch: 1, step: 149, loss: 75.870506 \n",
      "Epoch: 1, step: 150, loss: 95.338684 \n",
      "Epoch: 1, step: 151, loss: 63.254921 \n",
      "Epoch: 1, step: 152, loss: 122.789749 \n",
      "Epoch: 1, step: 153, loss: 151.677597 \n",
      "Epoch: 1, step: 154, loss: 115.651985 \n",
      "Epoch: 1, step: 155, loss: 182.798386 \n",
      "Epoch: 1, step: 156, loss: 46.353310 \n",
      "Epoch: 1, step: 157, loss: 112.979950 \n",
      "Epoch: 1, step: 158, loss: 31.076710 \n",
      "Epoch: 1, step: 159, loss: 186.885910 \n",
      "Epoch: 1, step: 160, loss: 69.665817 \n",
      "Epoch: 1, step: 161, loss: 70.254623 \n",
      "Epoch: 1, step: 162, loss: 152.384842 \n",
      "Epoch: 1, step: 163, loss: 195.715973 \n",
      "Epoch: 1, step: 164, loss: 66.005142 \n",
      "Epoch: 1, step: 165, loss: 2.959129 \n",
      "Epoch: 1, step: 166, loss: 132.632187 \n",
      "Epoch: 1, step: 167, loss: 61.194443 \n",
      "Epoch: 1, step: 168, loss: 48.312027 \n",
      "Epoch: 1, step: 169, loss: 103.130539 \n",
      "Epoch: 1, step: 170, loss: 118.869171 \n",
      "Epoch: 1, step: 171, loss: 118.139091 \n",
      "Epoch: 1, step: 172, loss: 51.710960 \n",
      "Epoch: 1, step: 173, loss: 81.818069 \n",
      "Epoch: 1, step: 174, loss: 132.108231 \n",
      "Epoch: 1, step: 175, loss: 35.391186 \n",
      "Epoch: 1, step: 176, loss: 39.059364 \n",
      "Epoch: 1, step: 177, loss: 16.998516 \n",
      "Epoch: 1, step: 178, loss: 18.084837 \n",
      "Epoch: 1, step: 179, loss: 27.120335 \n",
      "Epoch: 1, step: 180, loss: 166.141891 \n",
      "Epoch: 1, step: 181, loss: 107.121635 \n",
      "Epoch: 1, step: 182, loss: 196.289291 \n",
      "Epoch: 1, step: 183, loss: 101.092545 \n",
      "Epoch: 1, step: 184, loss: 38.376335 \n",
      "Epoch: 1, step: 185, loss: 7.765849 \n",
      "Epoch: 1, step: 186, loss: 27.276978 \n",
      "Epoch: 1, step: 187, loss: 138.885422 \n",
      "Epoch: 1, step: 188, loss: 62.971149 \n",
      "Epoch: 1, step: 189, loss: 81.702751 \n",
      "Epoch: 1, step: 190, loss: 58.016560 \n",
      "Epoch: 1, step: 191, loss: 25.550587 \n",
      "Epoch: 1, step: 192, loss: 80.138145 \n",
      "Epoch: 1, step: 193, loss: 45.310181 \n",
      "Epoch: 1, step: 194, loss: 183.922791 \n",
      "Epoch: 1, step: 195, loss: 134.320419 \n",
      "Epoch: 1, step: 196, loss: 67.937370 \n",
      "Epoch: 1, step: 197, loss: 78.703293 \n",
      "Epoch: 1, step: 198, loss: 12.919408 \n",
      "Epoch: 1, step: 199, loss: 86.795746 \n",
      "Epoch: 1, step: 200, loss: 59.996632 \n",
      "Epoch: 1, step: 201, loss: 121.885002 \n",
      "Epoch: 1, step: 202, loss: 97.476570 \n",
      "Epoch: 1, step: 203, loss: 51.710258 \n",
      "Epoch: 1, step: 204, loss: 97.171547 \n",
      "Epoch: 1, step: 205, loss: 35.987068 \n",
      "Epoch: 1, step: 206, loss: 27.643816 \n",
      "Epoch: 1, step: 207, loss: 69.352058 \n",
      "Epoch: 1, step: 208, loss: 146.150528 \n",
      "Epoch: 1, step: 209, loss: 40.001198 \n",
      "Epoch: 1, step: 210, loss: 153.447968 \n",
      "Epoch: 1, step: 211, loss: 1.060191 \n",
      "Epoch: 1, step: 212, loss: 62.102783 \n",
      "Epoch: 1, step: 213, loss: 85.840652 \n",
      "Epoch: 1, step: 214, loss: 107.454727 \n",
      "Epoch: 1, step: 215, loss: 41.244102 \n",
      "Epoch: 1, step: 216, loss: 60.303139 \n",
      "Epoch: 1, step: 217, loss: 56.599758 \n",
      "Epoch: 1, step: 218, loss: 10.965876 \n",
      "Epoch: 1, step: 219, loss: 58.251247 \n",
      "Epoch: 1, step: 220, loss: 18.581976 \n",
      "Epoch: 1, step: 221, loss: 99.574265 \n",
      "Epoch: 1, step: 222, loss: 136.380249 \n",
      "Epoch: 1, step: 223, loss: 6.426828 \n",
      "Epoch: 1, step: 224, loss: 30.185183 \n",
      "Epoch: 1, step: 225, loss: 33.384560 \n",
      "Epoch: 1, step: 226, loss: 22.719650 \n",
      "Epoch: 1, step: 227, loss: 68.232811 \n",
      "Epoch: 1, step: 228, loss: 20.520504 \n",
      "Epoch: 1, step: 229, loss: 53.660606 \n",
      "Epoch: 1, step: 230, loss: 81.341621 \n",
      "Epoch: 1, step: 231, loss: 38.443512 \n",
      "Epoch: 1, step: 232, loss: 55.767719 \n",
      "Epoch: 1, step: 233, loss: 12.708223 \n",
      "Epoch: 1, step: 234, loss: 28.245663 \n",
      "Epoch: 1, step: 235, loss: 68.175781 \n",
      "Epoch: 1, step: 236, loss: 40.169983 \n",
      "Epoch: 1, step: 237, loss: 29.064081 \n",
      "Epoch: 1, step: 238, loss: 3.116290 \n",
      "Epoch: 1, step: 239, loss: 35.474804 \n",
      "Epoch: 1, step: 240, loss: 33.175060 \n",
      "Epoch: 1, step: 241, loss: 44.932426 \n",
      "Epoch: 1, step: 242, loss: 122.008713 \n",
      "Epoch: 1, step: 243, loss: 78.380203 \n",
      "Epoch: 1, step: 244, loss: 80.401169 \n",
      "Epoch: 1, step: 245, loss: 7.689950 \n",
      "Epoch: 1, step: 246, loss: 71.898003 \n",
      "Epoch: 1, step: 247, loss: 95.127197 \n",
      "Epoch: 1, step: 248, loss: 66.385002 \n",
      "Epoch: 1, step: 249, loss: 129.355377 \n",
      "Epoch: 1, step: 250, loss: 86.674835 \n",
      "Epoch: 1, step: 251, loss: 13.415503 \n",
      "Epoch: 2, step: 252, loss: 20.456961 \n",
      "Epoch: 2, step: 253, loss: 40.224827 \n",
      "Epoch: 2, step: 254, loss: 30.759508 \n",
      "Epoch: 2, step: 255, loss: 29.958187 \n",
      "Epoch: 2, step: 256, loss: 60.775585 \n",
      "Epoch: 2, step: 257, loss: 6.505734 \n",
      "Epoch: 2, step: 258, loss: 107.904419 \n",
      "Epoch: 2, step: 259, loss: 158.047913 \n",
      "Epoch: 2, step: 260, loss: 129.717438 \n",
      "Epoch: 2, step: 261, loss: 97.355560 \n",
      "Epoch: 2, step: 262, loss: 101.560822 \n",
      "Epoch: 2, step: 263, loss: 104.901993 \n",
      "Epoch: 2, step: 264, loss: 92.063400 \n",
      "Epoch: 2, step: 265, loss: 41.230724 \n",
      "Epoch: 2, step: 266, loss: 1.591391 \n",
      "Epoch: 2, step: 267, loss: 37.967575 \n",
      "Epoch: 2, step: 268, loss: 44.479668 \n",
      "Epoch: 2, step: 269, loss: 34.597645 \n",
      "Epoch: 2, step: 270, loss: 41.827942 \n",
      "Epoch: 2, step: 271, loss: 39.378052 \n",
      "Epoch: 2, step: 272, loss: 60.235031 \n",
      "Epoch: 2, step: 273, loss: 28.814066 \n",
      "Epoch: 2, step: 274, loss: 84.594002 \n",
      "Epoch: 2, step: 275, loss: 52.569901 \n",
      "Epoch: 2, step: 276, loss: 53.995975 \n",
      "Epoch: 2, step: 277, loss: 57.534492 \n",
      "Epoch: 2, step: 278, loss: 77.563347 \n",
      "Epoch: 2, step: 279, loss: 136.309830 \n",
      "Epoch: 2, step: 280, loss: 97.153397 \n",
      "Epoch: 2, step: 281, loss: 168.373413 \n",
      "Epoch: 2, step: 282, loss: 30.561745 \n",
      "Epoch: 2, step: 283, loss: 82.694321 \n",
      "Epoch: 2, step: 284, loss: 11.919171 \n",
      "Epoch: 2, step: 285, loss: 140.176636 \n",
      "Epoch: 2, step: 286, loss: 45.015263 \n",
      "Epoch: 2, step: 287, loss: 67.165565 \n",
      "Epoch: 2, step: 288, loss: 127.316879 \n",
      "Epoch: 2, step: 289, loss: 190.217758 \n",
      "Epoch: 2, step: 290, loss: 54.841404 \n",
      "Epoch: 2, step: 291, loss: 0.216858 \n",
      "Epoch: 2, step: 292, loss: 115.623894 \n",
      "Epoch: 2, step: 293, loss: 39.701214 \n",
      "Epoch: 2, step: 294, loss: 25.302866 \n",
      "Epoch: 2, step: 295, loss: 82.066902 \n",
      "Epoch: 2, step: 296, loss: 95.386459 \n",
      "Epoch: 2, step: 297, loss: 93.198967 \n",
      "Epoch: 2, step: 298, loss: 31.667984 \n",
      "Epoch: 2, step: 299, loss: 66.860695 \n",
      "Epoch: 2, step: 300, loss: 87.449516 \n",
      "Epoch: 2, step: 301, loss: 30.368250 \n",
      "Epoch: 2, step: 302, loss: 23.126101 \n",
      "Epoch: 2, step: 303, loss: 5.588311 \n",
      "Epoch: 2, step: 304, loss: 11.151811 \n",
      "Epoch: 2, step: 305, loss: 16.073597 \n",
      "Epoch: 2, step: 306, loss: 120.994484 \n",
      "Epoch: 2, step: 307, loss: 75.439262 \n",
      "Epoch: 2, step: 308, loss: 150.934143 \n",
      "Epoch: 2, step: 309, loss: 60.580383 \n",
      "Epoch: 2, step: 310, loss: 19.599190 \n",
      "Epoch: 2, step: 311, loss: 3.930721 \n",
      "Epoch: 2, step: 312, loss: 15.887148 \n",
      "Epoch: 2, step: 313, loss: 122.359360 \n",
      "Epoch: 2, step: 314, loss: 51.926380 \n",
      "Epoch: 2, step: 315, loss: 63.810986 \n",
      "Epoch: 2, step: 316, loss: 56.414589 \n",
      "Epoch: 2, step: 317, loss: 10.868152 \n",
      "Epoch: 2, step: 318, loss: 65.928200 \n",
      "Epoch: 2, step: 319, loss: 33.761002 \n",
      "Epoch: 2, step: 320, loss: 169.687134 \n",
      "Epoch: 2, step: 321, loss: 104.412399 \n",
      "Epoch: 2, step: 322, loss: 63.700054 \n",
      "Epoch: 2, step: 323, loss: 54.830322 \n",
      "Epoch: 2, step: 324, loss: 7.051775 \n",
      "Epoch: 2, step: 325, loss: 66.069298 \n",
      "Epoch: 2, step: 326, loss: 53.174892 \n",
      "Epoch: 2, step: 327, loss: 100.755272 \n",
      "Epoch: 2, step: 328, loss: 65.690224 \n",
      "Epoch: 2, step: 329, loss: 48.517593 \n",
      "Epoch: 2, step: 330, loss: 64.975861 \n",
      "Epoch: 2, step: 331, loss: 12.366780 \n",
      "Epoch: 2, step: 332, loss: 11.611547 \n",
      "Epoch: 2, step: 333, loss: 50.841389 \n",
      "Epoch: 2, step: 334, loss: 126.198517 \n",
      "Epoch: 2, step: 335, loss: 33.515839 \n",
      "Epoch: 2, step: 336, loss: 138.359970 \n",
      "Epoch: 2, step: 337, loss: 0.005798 \n",
      "Epoch: 2, step: 338, loss: 58.102230 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, step: 339, loss: 74.562393 \n",
      "Epoch: 2, step: 340, loss: 95.968666 \n",
      "Epoch: 2, step: 341, loss: 31.784515 \n",
      "Epoch: 2, step: 342, loss: 53.929775 \n",
      "Epoch: 2, step: 343, loss: 54.766060 \n",
      "Epoch: 2, step: 344, loss: 0.893663 \n",
      "Epoch: 2, step: 345, loss: 36.268486 \n",
      "Epoch: 2, step: 346, loss: 5.092955 \n",
      "Epoch: 2, step: 347, loss: 79.986008 \n",
      "Epoch: 2, step: 348, loss: 114.023643 \n",
      "Epoch: 2, step: 349, loss: 0.432268 \n",
      "Epoch: 2, step: 350, loss: 24.947395 \n",
      "Epoch: 2, step: 351, loss: 29.198671 \n",
      "Epoch: 2, step: 352, loss: 16.074434 \n",
      "Epoch: 2, step: 353, loss: 42.453430 \n",
      "Epoch: 2, step: 354, loss: 5.276576 \n",
      "Epoch: 2, step: 355, loss: 24.746326 \n",
      "Epoch: 2, step: 356, loss: 55.824570 \n",
      "Epoch: 2, step: 357, loss: 24.485044 \n",
      "Epoch: 2, step: 358, loss: 39.277912 \n",
      "Epoch: 2, step: 359, loss: 6.667773 \n",
      "Epoch: 2, step: 360, loss: 35.444794 \n",
      "Epoch: 2, step: 361, loss: 41.480236 \n",
      "Epoch: 2, step: 362, loss: 39.187111 \n",
      "Epoch: 2, step: 363, loss: 14.931925 \n",
      "Epoch: 2, step: 364, loss: 0.880635 \n",
      "Epoch: 2, step: 365, loss: 20.276432 \n",
      "Epoch: 2, step: 366, loss: 21.951630 \n",
      "Epoch: 2, step: 367, loss: 36.890373 \n",
      "Epoch: 2, step: 368, loss: 109.528572 \n",
      "Epoch: 2, step: 369, loss: 60.607899 \n",
      "Epoch: 2, step: 370, loss: 78.961090 \n",
      "Epoch: 2, step: 371, loss: 3.867126 \n",
      "Epoch: 2, step: 372, loss: 59.542110 \n",
      "Epoch: 2, step: 373, loss: 71.363602 \n",
      "Epoch: 2, step: 374, loss: 52.665607 \n",
      "Epoch: 2, step: 375, loss: 108.206619 \n",
      "Epoch: 2, step: 376, loss: 70.090851 \n",
      "Epoch: 2, step: 377, loss: 9.755923 \n",
      "Epoch: 3, step: 378, loss: 11.182487 \n",
      "Epoch: 3, step: 379, loss: 28.597626 \n",
      "Epoch: 3, step: 380, loss: 27.299669 \n",
      "Epoch: 3, step: 381, loss: 24.770702 \n",
      "Epoch: 3, step: 382, loss: 40.203499 \n",
      "Epoch: 3, step: 383, loss: 3.246742 \n",
      "Epoch: 3, step: 384, loss: 88.763313 \n",
      "Epoch: 3, step: 385, loss: 140.715775 \n",
      "Epoch: 3, step: 386, loss: 104.202118 \n",
      "Epoch: 3, step: 387, loss: 96.728180 \n",
      "Epoch: 3, step: 388, loss: 81.795113 \n",
      "Epoch: 3, step: 389, loss: 69.167595 \n",
      "Epoch: 3, step: 390, loss: 66.613678 \n",
      "Epoch: 3, step: 391, loss: 29.786638 \n",
      "Epoch: 3, step: 392, loss: 0.120228 \n",
      "Epoch: 3, step: 393, loss: 32.228691 \n",
      "Epoch: 3, step: 394, loss: 33.796494 \n",
      "Epoch: 3, step: 395, loss: 27.050629 \n",
      "Epoch: 3, step: 396, loss: 35.610199 \n",
      "Epoch: 3, step: 397, loss: 34.705807 \n",
      "Epoch: 3, step: 398, loss: 55.452038 \n",
      "Epoch: 3, step: 399, loss: 18.895918 \n",
      "Epoch: 3, step: 400, loss: 68.538666 \n",
      "Epoch: 3, step: 401, loss: 35.563934 \n",
      "Epoch: 3, step: 402, loss: 32.411591 \n",
      "Epoch: 3, step: 403, loss: 52.289291 \n",
      "Epoch: 3, step: 404, loss: 60.808548 \n",
      "Epoch: 3, step: 405, loss: 122.488609 \n",
      "Epoch: 3, step: 406, loss: 80.502899 \n",
      "Epoch: 3, step: 407, loss: 152.943451 \n",
      "Epoch: 3, step: 408, loss: 24.088852 \n",
      "Epoch: 3, step: 409, loss: 70.987579 \n",
      "Epoch: 3, step: 410, loss: 8.340545 \n",
      "Epoch: 3, step: 411, loss: 101.162399 \n",
      "Epoch: 3, step: 412, loss: 30.170853 \n",
      "Epoch: 3, step: 413, loss: 67.078545 \n",
      "Epoch: 3, step: 414, loss: 108.378250 \n",
      "Epoch: 3, step: 415, loss: 174.284775 \n",
      "Epoch: 3, step: 416, loss: 47.353645 \n",
      "Epoch: 3, step: 417, loss: 0.108629 \n",
      "Epoch: 3, step: 418, loss: 103.898544 \n",
      "Epoch: 3, step: 419, loss: 16.001591 \n",
      "Epoch: 3, step: 420, loss: 17.640299 \n",
      "Epoch: 3, step: 421, loss: 71.796143 \n",
      "Epoch: 3, step: 422, loss: 81.214867 \n",
      "Epoch: 3, step: 423, loss: 79.728615 \n",
      "Epoch: 3, step: 424, loss: 19.750538 \n",
      "Epoch: 3, step: 425, loss: 58.803162 \n",
      "Epoch: 3, step: 426, loss: 61.322430 \n",
      "Epoch: 3, step: 427, loss: 26.708122 \n",
      "Epoch: 3, step: 428, loss: 15.199989 \n",
      "Epoch: 3, step: 429, loss: 0.885797 \n",
      "Epoch: 3, step: 430, loss: 10.882998 \n",
      "Epoch: 3, step: 431, loss: 10.544487 \n",
      "Epoch: 3, step: 432, loss: 90.598831 \n",
      "Epoch: 3, step: 433, loss: 60.936558 \n",
      "Epoch: 3, step: 434, loss: 119.316116 \n",
      "Epoch: 3, step: 435, loss: 33.514271 \n",
      "Epoch: 3, step: 436, loss: 10.703774 \n",
      "Epoch: 3, step: 437, loss: 1.748776 \n",
      "Epoch: 3, step: 438, loss: 8.354492 \n",
      "Epoch: 3, step: 439, loss: 112.118675 \n",
      "Epoch: 3, step: 440, loss: 44.871918 \n",
      "Epoch: 3, step: 441, loss: 52.366024 \n",
      "Epoch: 3, step: 442, loss: 50.445194 \n",
      "Epoch: 3, step: 443, loss: 3.675349 \n",
      "Epoch: 3, step: 444, loss: 56.972565 \n",
      "Epoch: 3, step: 445, loss: 26.779005 \n",
      "Epoch: 3, step: 446, loss: 153.376709 \n",
      "Epoch: 3, step: 447, loss: 78.803459 \n",
      "Epoch: 3, step: 448, loss: 57.919647 \n",
      "Epoch: 3, step: 449, loss: 40.139008 \n",
      "Epoch: 3, step: 450, loss: 5.714846 \n",
      "Epoch: 3, step: 451, loss: 54.629055 \n",
      "Epoch: 3, step: 452, loss: 46.425270 \n",
      "Epoch: 3, step: 453, loss: 85.099159 \n",
      "Epoch: 3, step: 454, loss: 48.294296 \n",
      "Epoch: 3, step: 455, loss: 46.193527 \n",
      "Epoch: 3, step: 456, loss: 43.607285 \n",
      "Epoch: 3, step: 457, loss: 6.913600 \n",
      "Epoch: 3, step: 458, loss: 4.779167 \n",
      "Epoch: 3, step: 459, loss: 42.081863 \n",
      "Epoch: 3, step: 460, loss: 111.034012 \n",
      "Epoch: 3, step: 461, loss: 28.937088 \n",
      "Epoch: 3, step: 462, loss: 122.773735 \n",
      "Epoch: 3, step: 463, loss: 0.003387 \n",
      "Epoch: 3, step: 464, loss: 54.265842 \n",
      "Epoch: 3, step: 465, loss: 63.367130 \n",
      "Epoch: 3, step: 466, loss: 75.526878 \n",
      "Epoch: 3, step: 467, loss: 26.742947 \n",
      "Epoch: 3, step: 468, loss: 49.445610 \n",
      "Epoch: 3, step: 469, loss: 51.058960 \n",
      "Epoch: 3, step: 470, loss: 0.420899 \n",
      "Epoch: 3, step: 471, loss: 32.662251 \n",
      "Epoch: 3, step: 472, loss: 2.897115 \n",
      "Epoch: 3, step: 473, loss: 69.292206 \n",
      "Epoch: 3, step: 474, loss: 99.722008 \n",
      "Epoch: 3, step: 475, loss: 0.039100 \n",
      "Epoch: 3, step: 476, loss: 21.002192 \n",
      "Epoch: 3, step: 477, loss: 26.077942 \n",
      "Epoch: 3, step: 478, loss: 12.087945 \n",
      "Epoch: 3, step: 479, loss: 31.369087 \n",
      "Epoch: 3, step: 480, loss: 2.036195 \n",
      "Epoch: 3, step: 481, loss: 12.480547 \n",
      "Epoch: 3, step: 482, loss: 41.234459 \n",
      "Epoch: 3, step: 483, loss: 14.495426 \n",
      "Epoch: 3, step: 484, loss: 27.714199 \n",
      "Epoch: 3, step: 485, loss: 4.950628 \n",
      "Epoch: 3, step: 486, loss: 35.043327 \n",
      "Epoch: 3, step: 487, loss: 30.419556 \n",
      "Epoch: 3, step: 488, loss: 36.085926 \n",
      "Epoch: 3, step: 489, loss: 9.214279 \n",
      "Epoch: 3, step: 490, loss: 0.458665 \n",
      "Epoch: 3, step: 491, loss: 12.931491 \n",
      "Epoch: 3, step: 492, loss: 15.004629 \n",
      "Epoch: 3, step: 493, loss: 31.211443 \n",
      "Epoch: 3, step: 494, loss: 95.438934 \n",
      "Epoch: 3, step: 495, loss: 47.873154 \n",
      "Epoch: 3, step: 496, loss: 74.359909 \n",
      "Epoch: 3, step: 497, loss: 1.785329 \n",
      "Epoch: 3, step: 498, loss: 49.718613 \n",
      "Epoch: 3, step: 499, loss: 63.583401 \n",
      "Epoch: 3, step: 500, loss: 40.609779 \n",
      "Epoch: 3, step: 501, loss: 90.972244 \n",
      "Epoch: 3, step: 502, loss: 58.430889 \n",
      "Epoch: 3, step: 503, loss: 9.379778 \n",
      "Epoch: 4, step: 504, loss: 6.954370 \n",
      "Epoch: 4, step: 505, loss: 22.976509 \n",
      "Epoch: 4, step: 506, loss: 23.593660 \n",
      "Epoch: 4, step: 507, loss: 20.742653 \n",
      "Epoch: 4, step: 508, loss: 28.494188 \n",
      "Epoch: 4, step: 509, loss: 2.386014 \n",
      "Epoch: 4, step: 510, loss: 73.423645 \n",
      "Epoch: 4, step: 511, loss: 126.511887 \n",
      "Epoch: 4, step: 512, loss: 84.630020 \n",
      "Epoch: 4, step: 513, loss: 93.652679 \n",
      "Epoch: 4, step: 514, loss: 67.050934 \n",
      "Epoch: 4, step: 515, loss: 44.057693 \n",
      "Epoch: 4, step: 516, loss: 54.176743 \n",
      "Epoch: 4, step: 517, loss: 23.997982 \n",
      "Epoch: 4, step: 518, loss: 0.018615 \n",
      "Epoch: 4, step: 519, loss: 26.285957 \n",
      "Epoch: 4, step: 520, loss: 24.645945 \n",
      "Epoch: 4, step: 521, loss: 22.246836 \n",
      "Epoch: 4, step: 522, loss: 29.435787 \n",
      "Epoch: 4, step: 523, loss: 30.171999 \n",
      "Epoch: 4, step: 524, loss: 49.859940 \n",
      "Epoch: 4, step: 525, loss: 12.125364 \n",
      "Epoch: 4, step: 526, loss: 57.303268 \n",
      "Epoch: 4, step: 527, loss: 22.938759 \n",
      "Epoch: 4, step: 528, loss: 17.454315 \n",
      "Epoch: 4, step: 529, loss: 46.881615 \n",
      "Epoch: 4, step: 530, loss: 50.700447 \n",
      "Epoch: 4, step: 531, loss: 110.511414 \n",
      "Epoch: 4, step: 532, loss: 69.274490 \n",
      "Epoch: 4, step: 533, loss: 142.695709 \n",
      "Epoch: 4, step: 534, loss: 17.574619 \n",
      "Epoch: 4, step: 535, loss: 61.105370 \n",
      "Epoch: 4, step: 536, loss: 6.952119 \n",
      "Epoch: 4, step: 537, loss: 70.726364 \n",
      "Epoch: 4, step: 538, loss: 20.479204 \n",
      "Epoch: 4, step: 539, loss: 64.601494 \n",
      "Epoch: 4, step: 540, loss: 91.693283 \n",
      "Epoch: 4, step: 541, loss: 154.893280 \n",
      "Epoch: 4, step: 542, loss: 41.052921 \n",
      "Epoch: 4, step: 543, loss: 0.117194 \n",
      "Epoch: 4, step: 544, loss: 95.607239 \n",
      "Epoch: 4, step: 545, loss: 4.669750 \n",
      "Epoch: 4, step: 546, loss: 12.951379 \n",
      "Epoch: 4, step: 547, loss: 62.821114 \n",
      "Epoch: 4, step: 548, loss: 70.716553 \n",
      "Epoch: 4, step: 549, loss: 69.677475 \n",
      "Epoch: 4, step: 550, loss: 9.728403 \n",
      "Epoch: 4, step: 551, loss: 52.447716 \n",
      "Epoch: 4, step: 552, loss: 45.625034 \n",
      "Epoch: 4, step: 553, loss: 22.852564 \n",
      "Epoch: 4, step: 554, loss: 10.174273 \n",
      "Epoch: 4, step: 555, loss: 0.325277 \n",
      "Epoch: 4, step: 556, loss: 12.590969 \n",
      "Epoch: 4, step: 557, loss: 6.519706 \n",
      "Epoch: 4, step: 558, loss: 69.755768 \n",
      "Epoch: 4, step: 559, loss: 53.050488 \n",
      "Epoch: 4, step: 560, loss: 94.130112 \n",
      "Epoch: 4, step: 561, loss: 15.865018 \n",
      "Epoch: 4, step: 562, loss: 6.069204 \n",
      "Epoch: 4, step: 563, loss: 0.453076 \n",
      "Epoch: 4, step: 564, loss: 3.487700 \n",
      "Epoch: 4, step: 565, loss: 103.824921 \n",
      "Epoch: 4, step: 566, loss: 38.459362 \n",
      "Epoch: 4, step: 567, loss: 47.981564 \n",
      "Epoch: 4, step: 568, loss: 44.040913 \n",
      "Epoch: 4, step: 569, loss: 1.903790 \n",
      "Epoch: 4, step: 570, loss: 49.060658 \n",
      "Epoch: 4, step: 571, loss: 22.319899 \n",
      "Epoch: 4, step: 572, loss: 136.371765 \n",
      "Epoch: 4, step: 573, loss: 57.914192 \n",
      "Epoch: 4, step: 574, loss: 50.380306 \n",
      "Epoch: 4, step: 575, loss: 31.442606 \n",
      "Epoch: 4, step: 576, loss: 4.531446 \n",
      "Epoch: 4, step: 577, loss: 45.952377 \n",
      "Epoch: 4, step: 578, loss: 40.407005 \n",
      "Epoch: 4, step: 579, loss: 76.971100 \n",
      "Epoch: 4, step: 580, loss: 39.531288 \n",
      "Epoch: 4, step: 581, loss: 40.548042 \n",
      "Epoch: 4, step: 582, loss: 27.172947 \n",
      "Epoch: 4, step: 583, loss: 5.077125 \n",
      "Epoch: 4, step: 584, loss: 1.599627 \n",
      "Epoch: 4, step: 585, loss: 36.605011 \n",
      "Epoch: 4, step: 586, loss: 99.531059 \n",
      "Epoch: 4, step: 587, loss: 24.041126 \n",
      "Epoch: 4, step: 588, loss: 106.784454 \n",
      "Epoch: 4, step: 589, loss: 0.004926 \n",
      "Epoch: 4, step: 590, loss: 50.194386 \n",
      "Epoch: 4, step: 591, loss: 52.939888 \n",
      "Epoch: 4, step: 592, loss: 55.251415 \n",
      "Epoch: 4, step: 593, loss: 22.573198 \n",
      "Epoch: 4, step: 594, loss: 45.019245 \n",
      "Epoch: 4, step: 595, loss: 46.349735 \n",
      "Epoch: 4, step: 596, loss: 0.368153 \n",
      "Epoch: 4, step: 597, loss: 28.829937 \n",
      "Epoch: 4, step: 598, loss: 2.698403 \n",
      "Epoch: 4, step: 599, loss: 63.260307 \n",
      "Epoch: 4, step: 600, loss: 88.159698 \n",
      "Epoch: 4, step: 601, loss: 0.019556 \n",
      "Epoch: 4, step: 602, loss: 18.716621 \n",
      "Epoch: 4, step: 603, loss: 23.369658 \n",
      "Epoch: 4, step: 604, loss: 9.546205 \n",
      "Epoch: 4, step: 605, loss: 22.867634 \n",
      "Epoch: 4, step: 606, loss: 4.774259 \n",
      "Epoch: 4, step: 607, loss: 4.432086 \n",
      "Epoch: 4, step: 608, loss: 32.449394 \n",
      "Epoch: 4, step: 609, loss: 9.283365 \n",
      "Epoch: 4, step: 610, loss: 21.126232 \n",
      "Epoch: 4, step: 611, loss: 4.684117 \n",
      "Epoch: 4, step: 612, loss: 31.065533 \n",
      "Epoch: 4, step: 613, loss: 23.805931 \n",
      "Epoch: 4, step: 614, loss: 32.570229 \n",
      "Epoch: 4, step: 615, loss: 5.316167 \n",
      "Epoch: 4, step: 616, loss: 0.297689 \n",
      "Epoch: 4, step: 617, loss: 10.163670 \n",
      "Epoch: 4, step: 618, loss: 9.779832 \n",
      "Epoch: 4, step: 619, loss: 25.917213 \n",
      "Epoch: 4, step: 620, loss: 81.455864 \n",
      "Epoch: 4, step: 621, loss: 35.389397 \n",
      "Epoch: 4, step: 622, loss: 68.136421 \n",
      "Epoch: 4, step: 623, loss: 1.487933 \n",
      "Epoch: 4, step: 624, loss: 40.574532 \n",
      "Epoch: 4, step: 625, loss: 58.337589 \n",
      "Epoch: 4, step: 626, loss: 31.787241 \n",
      "Epoch: 4, step: 627, loss: 75.899506 \n",
      "Epoch: 4, step: 628, loss: 49.231102 \n",
      "Epoch: 4, step: 629, loss: 9.397795 \n",
      "Epoch: 5, step: 630, loss: 5.404722 \n",
      "Epoch: 5, step: 631, loss: 19.300898 \n",
      "Epoch: 5, step: 632, loss: 20.123096 \n",
      "Epoch: 5, step: 633, loss: 17.586025 \n",
      "Epoch: 5, step: 634, loss: 20.534958 \n",
      "Epoch: 5, step: 635, loss: 2.155877 \n",
      "Epoch: 5, step: 636, loss: 59.435707 \n",
      "Epoch: 5, step: 637, loss: 113.389053 \n",
      "Epoch: 5, step: 638, loss: 67.541809 \n",
      "Epoch: 5, step: 639, loss: 88.857040 \n",
      "Epoch: 5, step: 640, loss: 57.262188 \n",
      "Epoch: 5, step: 641, loss: 26.999640 \n",
      "Epoch: 5, step: 642, loss: 46.332352 \n",
      "Epoch: 5, step: 643, loss: 21.090162 \n",
      "Epoch: 5, step: 644, loss: 0.006528 \n",
      "Epoch: 5, step: 645, loss: 19.948603 \n",
      "Epoch: 5, step: 646, loss: 17.832779 \n",
      "Epoch: 5, step: 647, loss: 19.914385 \n",
      "Epoch: 5, step: 648, loss: 23.307508 \n",
      "Epoch: 5, step: 649, loss: 25.811117 \n",
      "Epoch: 5, step: 650, loss: 43.298977 \n",
      "Epoch: 5, step: 651, loss: 7.003978 \n",
      "Epoch: 5, step: 652, loss: 48.889122 \n",
      "Epoch: 5, step: 653, loss: 15.072557 \n",
      "Epoch: 5, step: 654, loss: 11.086893 \n",
      "Epoch: 5, step: 655, loss: 42.087246 \n",
      "Epoch: 5, step: 656, loss: 42.961136 \n",
      "Epoch: 5, step: 657, loss: 100.457550 \n",
      "Epoch: 5, step: 658, loss: 59.833664 \n",
      "Epoch: 5, step: 659, loss: 132.872604 \n",
      "Epoch: 5, step: 660, loss: 12.550849 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, step: 661, loss: 52.090515 \n",
      "Epoch: 5, step: 662, loss: 6.497435 \n",
      "Epoch: 5, step: 663, loss: 49.390697 \n",
      "Epoch: 5, step: 664, loss: 13.658253 \n",
      "Epoch: 5, step: 665, loss: 60.592060 \n",
      "Epoch: 5, step: 666, loss: 78.104271 \n",
      "Epoch: 5, step: 667, loss: 134.803207 \n",
      "Epoch: 5, step: 668, loss: 35.247746 \n",
      "Epoch: 5, step: 669, loss: 0.174694 \n",
      "Epoch: 5, step: 670, loss: 88.410004 \n",
      "Epoch: 5, step: 671, loss: 0.338308 \n",
      "Epoch: 5, step: 672, loss: 11.212546 \n",
      "Epoch: 5, step: 673, loss: 54.511585 \n",
      "Epoch: 5, step: 674, loss: 63.029167 \n",
      "Epoch: 5, step: 675, loss: 62.199371 \n",
      "Epoch: 5, step: 676, loss: 2.004107 \n",
      "Epoch: 5, step: 677, loss: 46.868847 \n",
      "Epoch: 5, step: 678, loss: 33.239120 \n",
      "Epoch: 5, step: 679, loss: 19.047379 \n",
      "Epoch: 5, step: 680, loss: 6.918561 \n",
      "Epoch: 5, step: 681, loss: 0.218017 \n",
      "Epoch: 5, step: 682, loss: 12.988924 \n",
      "Epoch: 5, step: 683, loss: 3.785741 \n",
      "Epoch: 5, step: 684, loss: 57.038433 \n",
      "Epoch: 5, step: 685, loss: 47.446526 \n",
      "Epoch: 5, step: 686, loss: 73.641113 \n",
      "Epoch: 5, step: 687, loss: 9.868065 \n",
      "Epoch: 5, step: 688, loss: 3.579328 \n",
      "Epoch: 5, step: 689, loss: 0.172249 \n",
      "Epoch: 5, step: 690, loss: 1.601223 \n",
      "Epoch: 5, step: 691, loss: 95.887192 \n",
      "Epoch: 5, step: 692, loss: 32.480633 \n",
      "Epoch: 5, step: 693, loss: 44.311623 \n",
      "Epoch: 5, step: 694, loss: 36.457844 \n",
      "Epoch: 5, step: 695, loss: 1.324733 \n",
      "Epoch: 5, step: 696, loss: 42.198250 \n",
      "Epoch: 5, step: 697, loss: 19.413736 \n",
      "Epoch: 5, step: 698, loss: 119.794121 \n",
      "Epoch: 5, step: 699, loss: 40.319504 \n",
      "Epoch: 5, step: 700, loss: 42.999107 \n",
      "Epoch: 5, step: 701, loss: 25.957266 \n",
      "Epoch: 5, step: 702, loss: 3.332206 \n",
      "Epoch: 5, step: 703, loss: 38.390156 \n",
      "Epoch: 5, step: 704, loss: 35.425587 \n",
      "Epoch: 5, step: 705, loss: 71.408173 \n",
      "Epoch: 5, step: 706, loss: 33.414230 \n",
      "Epoch: 5, step: 707, loss: 33.505028 \n",
      "Epoch: 5, step: 708, loss: 15.790165 \n",
      "Epoch: 5, step: 709, loss: 3.650705 \n",
      "Epoch: 5, step: 710, loss: 1.984147 \n",
      "Epoch: 5, step: 711, loss: 32.349525 \n",
      "Epoch: 5, step: 712, loss: 89.313858 \n",
      "Epoch: 5, step: 713, loss: 19.128494 \n",
      "Epoch: 5, step: 714, loss: 92.412674 \n",
      "Epoch: 5, step: 715, loss: 0.009865 \n",
      "Epoch: 5, step: 716, loss: 46.327045 \n",
      "Epoch: 5, step: 717, loss: 43.075821 \n",
      "Epoch: 5, step: 718, loss: 39.125343 \n",
      "Epoch: 5, step: 719, loss: 19.191814 \n",
      "Epoch: 5, step: 720, loss: 40.850086 \n",
      "Epoch: 5, step: 721, loss: 41.509468 \n",
      "Epoch: 5, step: 722, loss: 0.392582 \n",
      "Epoch: 5, step: 723, loss: 24.440742 \n",
      "Epoch: 5, step: 724, loss: 1.521482 \n",
      "Epoch: 5, step: 725, loss: 58.412605 \n",
      "Epoch: 5, step: 726, loss: 78.356880 \n",
      "Epoch: 5, step: 727, loss: 0.027416 \n",
      "Epoch: 5, step: 728, loss: 17.235474 \n",
      "Epoch: 5, step: 729, loss: 21.082823 \n",
      "Epoch: 5, step: 730, loss: 7.597072 \n",
      "Epoch: 5, step: 731, loss: 16.191694 \n",
      "Epoch: 5, step: 732, loss: 6.854689 \n",
      "Epoch: 5, step: 733, loss: 0.784766 \n",
      "Epoch: 5, step: 734, loss: 27.289320 \n",
      "Epoch: 5, step: 735, loss: 7.827728 \n",
      "Epoch: 5, step: 736, loss: 16.402348 \n",
      "Epoch: 5, step: 737, loss: 3.908843 \n",
      "Epoch: 5, step: 738, loss: 25.279001 \n",
      "Epoch: 5, step: 739, loss: 19.160297 \n",
      "Epoch: 5, step: 740, loss: 28.335808 \n",
      "Epoch: 5, step: 741, loss: 2.800099 \n",
      "Epoch: 5, step: 742, loss: 0.217813 \n",
      "Epoch: 5, step: 743, loss: 7.972600 \n",
      "Epoch: 5, step: 744, loss: 5.597588 \n",
      "Epoch: 5, step: 745, loss: 21.115883 \n",
      "Epoch: 5, step: 746, loss: 68.259613 \n",
      "Epoch: 5, step: 747, loss: 24.446545 \n",
      "Epoch: 5, step: 748, loss: 61.193497 \n",
      "Epoch: 5, step: 749, loss: 1.545424 \n",
      "Epoch: 5, step: 750, loss: 32.510231 \n",
      "Epoch: 5, step: 751, loss: 52.888329 \n",
      "Epoch: 5, step: 752, loss: 26.021908 \n",
      "Epoch: 5, step: 753, loss: 63.471626 \n",
      "Epoch: 5, step: 754, loss: 41.293625 \n",
      "Epoch: 5, step: 755, loss: 9.316604 \n",
      "Epoch: 6, step: 756, loss: 4.557265 \n",
      "Epoch: 6, step: 757, loss: 15.851964 \n",
      "Epoch: 6, step: 758, loss: 16.315681 \n",
      "Epoch: 6, step: 759, loss: 14.889856 \n",
      "Epoch: 6, step: 760, loss: 14.698795 \n",
      "Epoch: 6, step: 761, loss: 1.725328 \n",
      "Epoch: 6, step: 762, loss: 44.947159 \n",
      "Epoch: 6, step: 763, loss: 101.052872 \n",
      "Epoch: 6, step: 764, loss: 53.025520 \n",
      "Epoch: 6, step: 765, loss: 83.703995 \n",
      "Epoch: 6, step: 766, loss: 48.842037 \n",
      "Epoch: 6, step: 767, loss: 17.584358 \n",
      "Epoch: 6, step: 768, loss: 41.245953 \n",
      "Epoch: 6, step: 769, loss: 18.574722 \n",
      "Epoch: 6, step: 770, loss: 0.003170 \n",
      "Epoch: 6, step: 771, loss: 13.998368 \n",
      "Epoch: 6, step: 772, loss: 11.978046 \n",
      "Epoch: 6, step: 773, loss: 17.804773 \n",
      "Epoch: 6, step: 774, loss: 17.427645 \n",
      "Epoch: 6, step: 775, loss: 21.907999 \n",
      "Epoch: 6, step: 776, loss: 36.838871 \n",
      "Epoch: 6, step: 777, loss: 2.826354 \n",
      "Epoch: 6, step: 778, loss: 44.828117 \n",
      "Epoch: 6, step: 779, loss: 12.918929 \n",
      "Epoch: 6, step: 780, loss: 8.710678 \n",
      "Epoch: 6, step: 781, loss: 37.037582 \n",
      "Epoch: 6, step: 782, loss: 36.414143 \n",
      "Epoch: 6, step: 783, loss: 91.215034 \n",
      "Epoch: 6, step: 784, loss: 51.324265 \n",
      "Epoch: 6, step: 785, loss: 123.081161 \n",
      "Epoch: 6, step: 786, loss: 9.222992 \n",
      "Epoch: 6, step: 787, loss: 44.031517 \n",
      "Epoch: 6, step: 788, loss: 6.175551 \n",
      "Epoch: 6, step: 789, loss: 34.456898 \n",
      "Epoch: 6, step: 790, loss: 8.246511 \n",
      "Epoch: 6, step: 791, loss: 55.213184 \n",
      "Epoch: 6, step: 792, loss: 66.515915 \n",
      "Epoch: 6, step: 793, loss: 116.267075 \n",
      "Epoch: 6, step: 794, loss: 29.947111 \n",
      "Epoch: 6, step: 795, loss: 0.227865 \n",
      "Epoch: 6, step: 796, loss: 80.957947 \n",
      "Epoch: 6, step: 797, loss: 0.275446 \n",
      "Epoch: 6, step: 798, loss: 9.881368 \n",
      "Epoch: 6, step: 799, loss: 47.502995 \n",
      "Epoch: 6, step: 800, loss: 57.230221 \n",
      "Epoch: 6, step: 801, loss: 56.304016 \n",
      "Epoch: 6, step: 802, loss: 0.087300 \n",
      "Epoch: 6, step: 803, loss: 41.795662 \n",
      "Epoch: 6, step: 804, loss: 23.032814 \n",
      "Epoch: 6, step: 805, loss: 15.580377 \n",
      "Epoch: 6, step: 806, loss: 4.772442 \n",
      "Epoch: 6, step: 807, loss: 0.196557 \n",
      "Epoch: 6, step: 808, loss: 12.908227 \n",
      "Epoch: 6, step: 809, loss: 2.262696 \n",
      "Epoch: 6, step: 810, loss: 49.645927 \n",
      "Epoch: 6, step: 811, loss: 42.146435 \n",
      "Epoch: 6, step: 812, loss: 57.768913 \n",
      "Epoch: 6, step: 813, loss: 6.331670 \n",
      "Epoch: 6, step: 814, loss: 2.070780 \n",
      "Epoch: 6, step: 815, loss: 0.166338 \n",
      "Epoch: 6, step: 816, loss: 1.212853 \n",
      "Epoch: 6, step: 817, loss: 87.433800 \n",
      "Epoch: 6, step: 818, loss: 25.823729 \n",
      "Epoch: 6, step: 819, loss: 40.592510 \n",
      "Epoch: 6, step: 820, loss: 30.030731 \n",
      "Epoch: 6, step: 821, loss: 1.025784 \n",
      "Epoch: 6, step: 822, loss: 35.138641 \n",
      "Epoch: 6, step: 823, loss: 17.228106 \n",
      "Epoch: 6, step: 824, loss: 104.008179 \n",
      "Epoch: 6, step: 825, loss: 24.063311 \n",
      "Epoch: 6, step: 826, loss: 36.507984 \n",
      "Epoch: 6, step: 827, loss: 20.724049 \n",
      "Epoch: 6, step: 828, loss: 2.687744 \n",
      "Epoch: 6, step: 829, loss: 32.248665 \n",
      "Epoch: 6, step: 830, loss: 31.072153 \n",
      "Epoch: 6, step: 831, loss: 66.371063 \n",
      "Epoch: 6, step: 832, loss: 28.142670 \n",
      "Epoch: 6, step: 833, loss: 27.822214 \n",
      "Epoch: 6, step: 834, loss: 8.984683 \n",
      "Epoch: 6, step: 835, loss: 2.477497 \n",
      "Epoch: 6, step: 836, loss: 2.250213 \n",
      "Epoch: 6, step: 837, loss: 28.735661 \n",
      "Epoch: 6, step: 838, loss: 80.168655 \n",
      "Epoch: 6, step: 839, loss: 14.414901 \n",
      "Epoch: 6, step: 840, loss: 79.479126 \n",
      "Epoch: 6, step: 841, loss: 0.025838 \n",
      "Epoch: 6, step: 842, loss: 42.848351 \n",
      "Epoch: 6, step: 843, loss: 33.901134 \n",
      "Epoch: 6, step: 844, loss: 23.673016 \n",
      "Epoch: 6, step: 845, loss: 16.641594 \n",
      "Epoch: 6, step: 846, loss: 37.471714 \n",
      "Epoch: 6, step: 847, loss: 36.716087 \n",
      "Epoch: 6, step: 848, loss: 0.499685 \n",
      "Epoch: 6, step: 849, loss: 19.606878 \n",
      "Epoch: 6, step: 850, loss: 0.698672 \n",
      "Epoch: 6, step: 851, loss: 53.599518 \n",
      "Epoch: 6, step: 852, loss: 68.567589 \n",
      "Epoch: 6, step: 853, loss: 0.034189 \n",
      "Epoch: 6, step: 854, loss: 16.427998 \n",
      "Epoch: 6, step: 855, loss: 18.765783 \n",
      "Epoch: 6, step: 856, loss: 5.642976 \n",
      "Epoch: 6, step: 857, loss: 10.807435 \n",
      "Epoch: 6, step: 858, loss: 7.567482 \n",
      "Epoch: 6, step: 859, loss: 0.307743 \n",
      "Epoch: 6, step: 860, loss: 25.562374 \n",
      "Epoch: 6, step: 861, loss: 7.280343 \n",
      "Epoch: 6, step: 862, loss: 12.668809 \n",
      "Epoch: 6, step: 863, loss: 3.292808 \n",
      "Epoch: 6, step: 864, loss: 21.004219 \n",
      "Epoch: 6, step: 865, loss: 14.974298 \n",
      "Epoch: 6, step: 866, loss: 24.624554 \n",
      "Epoch: 6, step: 867, loss: 1.577693 \n",
      "Epoch: 6, step: 868, loss: 0.170579 \n",
      "Epoch: 6, step: 869, loss: 5.969180 \n",
      "Epoch: 6, step: 870, loss: 2.887621 \n",
      "Epoch: 6, step: 871, loss: 17.964170 \n",
      "Epoch: 6, step: 872, loss: 56.461926 \n",
      "Epoch: 6, step: 873, loss: 16.403421 \n",
      "Epoch: 6, step: 874, loss: 54.669292 \n",
      "Epoch: 6, step: 875, loss: 1.815177 \n",
      "Epoch: 6, step: 876, loss: 26.518179 \n",
      "Epoch: 6, step: 877, loss: 47.727203 \n",
      "Epoch: 6, step: 878, loss: 20.948172 \n",
      "Epoch: 6, step: 879, loss: 52.763023 \n",
      "Epoch: 6, step: 880, loss: 34.226044 \n",
      "Epoch: 6, step: 881, loss: 9.212898 \n",
      "Epoch: 7, step: 882, loss: 3.722182 \n",
      "Epoch: 7, step: 883, loss: 12.844041 \n",
      "Epoch: 7, step: 884, loss: 13.180215 \n",
      "Epoch: 7, step: 885, loss: 12.415333 \n",
      "Epoch: 7, step: 886, loss: 9.503948 \n",
      "Epoch: 7, step: 887, loss: 1.364088 \n",
      "Epoch: 7, step: 888, loss: 32.653996 \n",
      "Epoch: 7, step: 889, loss: 89.911804 \n",
      "Epoch: 7, step: 890, loss: 40.142910 \n",
      "Epoch: 7, step: 891, loss: 78.774796 \n",
      "Epoch: 7, step: 892, loss: 42.330223 \n",
      "Epoch: 7, step: 893, loss: 10.529973 \n",
      "Epoch: 7, step: 894, loss: 37.209099 \n",
      "Epoch: 7, step: 895, loss: 16.124136 \n",
      "Epoch: 7, step: 896, loss: 0.001888 \n",
      "Epoch: 7, step: 897, loss: 8.939006 \n",
      "Epoch: 7, step: 898, loss: 7.397977 \n",
      "Epoch: 7, step: 899, loss: 15.957419 \n",
      "Epoch: 7, step: 900, loss: 12.609662 \n",
      "Epoch: 7, step: 901, loss: 18.302799 \n",
      "Epoch: 7, step: 902, loss: 30.344004 \n",
      "Epoch: 7, step: 903, loss: 1.110029 \n",
      "Epoch: 7, step: 904, loss: 41.668873 \n",
      "Epoch: 7, step: 905, loss: 11.434704 \n",
      "Epoch: 7, step: 906, loss: 6.111742 \n",
      "Epoch: 7, step: 907, loss: 31.808605 \n",
      "Epoch: 7, step: 908, loss: 30.639614 \n",
      "Epoch: 7, step: 909, loss: 83.794487 \n",
      "Epoch: 7, step: 910, loss: 43.109478 \n",
      "Epoch: 7, step: 911, loss: 114.040848 \n",
      "Epoch: 7, step: 912, loss: 8.225817 \n",
      "Epoch: 7, step: 913, loss: 37.473259 \n",
      "Epoch: 7, step: 914, loss: 5.737206 \n",
      "Epoch: 7, step: 915, loss: 22.769547 \n",
      "Epoch: 7, step: 916, loss: 4.529060 \n",
      "Epoch: 7, step: 917, loss: 49.308746 \n",
      "Epoch: 7, step: 918, loss: 56.358158 \n",
      "Epoch: 7, step: 919, loss: 99.216858 \n",
      "Epoch: 7, step: 920, loss: 24.875647 \n",
      "Epoch: 7, step: 921, loss: 0.220117 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, step: 922, loss: 73.482338 \n",
      "Epoch: 7, step: 923, loss: 0.286778 \n",
      "Epoch: 7, step: 924, loss: 8.609140 \n",
      "Epoch: 7, step: 925, loss: 42.313396 \n",
      "Epoch: 7, step: 926, loss: 52.369671 \n",
      "Epoch: 7, step: 927, loss: 50.787037 \n",
      "Epoch: 7, step: 928, loss: 0.064136 \n",
      "Epoch: 7, step: 929, loss: 37.671146 \n",
      "Epoch: 7, step: 930, loss: 15.461261 \n",
      "Epoch: 7, step: 931, loss: 12.674550 \n",
      "Epoch: 7, step: 932, loss: 2.986162 \n",
      "Epoch: 7, step: 933, loss: 0.270014 \n",
      "Epoch: 7, step: 934, loss: 11.941789 \n",
      "Epoch: 7, step: 935, loss: 1.479437 \n",
      "Epoch: 7, step: 936, loss: 43.774857 \n",
      "Epoch: 7, step: 937, loss: 36.488197 \n",
      "Epoch: 7, step: 938, loss: 46.469341 \n",
      "Epoch: 7, step: 939, loss: 3.803274 \n",
      "Epoch: 7, step: 940, loss: 1.206325 \n",
      "Epoch: 7, step: 941, loss: 0.244662 \n",
      "Epoch: 7, step: 942, loss: 1.486928 \n",
      "Epoch: 7, step: 943, loss: 78.858177 \n",
      "Epoch: 7, step: 944, loss: 18.936865 \n",
      "Epoch: 7, step: 945, loss: 36.809414 \n",
      "Epoch: 7, step: 946, loss: 23.601091 \n",
      "Epoch: 7, step: 947, loss: 0.811280 \n",
      "Epoch: 7, step: 948, loss: 28.168924 \n",
      "Epoch: 7, step: 949, loss: 15.345424 \n",
      "Epoch: 7, step: 950, loss: 89.464218 \n",
      "Epoch: 7, step: 951, loss: 12.079561 \n",
      "Epoch: 7, step: 952, loss: 30.392736 \n",
      "Epoch: 7, step: 953, loss: 15.567873 \n",
      "Epoch: 7, step: 954, loss: 1.961865 \n",
      "Epoch: 7, step: 955, loss: 26.384212 \n",
      "Epoch: 7, step: 956, loss: 27.385895 \n",
      "Epoch: 7, step: 957, loss: 61.290527 \n",
      "Epoch: 7, step: 958, loss: 23.594564 \n",
      "Epoch: 7, step: 959, loss: 21.835297 \n",
      "Epoch: 7, step: 960, loss: 6.528650 \n",
      "Epoch: 7, step: 961, loss: 1.434205 \n",
      "Epoch: 7, step: 962, loss: 1.943954 \n",
      "Epoch: 7, step: 963, loss: 25.904804 \n",
      "Epoch: 7, step: 964, loss: 71.534882 \n",
      "Epoch: 7, step: 965, loss: 10.230683 \n",
      "Epoch: 7, step: 966, loss: 68.231781 \n",
      "Epoch: 7, step: 967, loss: 0.062620 \n",
      "Epoch: 7, step: 968, loss: 40.168186 \n",
      "Epoch: 7, step: 969, loss: 26.133490 \n",
      "Epoch: 7, step: 970, loss: 11.595931 \n",
      "Epoch: 7, step: 971, loss: 14.348742 \n",
      "Epoch: 7, step: 972, loss: 34.544827 \n",
      "Epoch: 7, step: 973, loss: 32.259491 \n",
      "Epoch: 7, step: 974, loss: 0.952140 \n",
      "Epoch: 7, step: 975, loss: 14.766184 \n",
      "Epoch: 7, step: 976, loss: 0.262391 \n",
      "Epoch: 7, step: 977, loss: 48.386406 \n",
      "Epoch: 7, step: 978, loss: 59.760807 \n",
      "Epoch: 7, step: 979, loss: 0.044736 \n",
      "Epoch: 7, step: 980, loss: 15.411221 \n",
      "Epoch: 7, step: 981, loss: 16.366537 \n",
      "Epoch: 7, step: 982, loss: 3.825561 \n",
      "Epoch: 7, step: 983, loss: 7.081370 \n",
      "Epoch: 7, step: 984, loss: 6.518981 \n",
      "Epoch: 7, step: 985, loss: 0.191004 \n",
      "Epoch: 7, step: 986, loss: 24.219242 \n",
      "Epoch: 7, step: 987, loss: 7.153149 \n",
      "Epoch: 7, step: 988, loss: 10.392673 \n",
      "Epoch: 7, step: 989, loss: 2.794869 \n",
      "Epoch: 7, step: 990, loss: 17.441114 \n",
      "Epoch: 7, step: 991, loss: 12.141762 \n",
      "Epoch: 7, step: 992, loss: 21.461046 \n",
      "Epoch: 7, step: 993, loss: 0.993782 \n",
      "Epoch: 7, step: 994, loss: 0.141650 \n",
      "Epoch: 7, step: 995, loss: 4.120578 \n",
      "Epoch: 7, step: 996, loss: 2.571597 \n",
      "Epoch: 7, step: 997, loss: 16.351198 \n",
      "Epoch: 7, step: 998, loss: 46.822655 \n",
      "Epoch: 7, step: 999, loss: 12.466032 \n",
      "Epoch: 7, step: 1000, loss: 49.194572 \n",
      "Epoch: 7, step: 1001, loss: 1.666668 \n",
      "Epoch: 7, step: 1002, loss: 21.649084 \n",
      "Epoch: 7, step: 1003, loss: 42.919365 \n",
      "Epoch: 7, step: 1004, loss: 16.912411 \n",
      "Epoch: 7, step: 1005, loss: 43.758175 \n",
      "Epoch: 7, step: 1006, loss: 27.905252 \n",
      "Epoch: 7, step: 1007, loss: 9.105506 \n",
      "Epoch: 8, step: 1008, loss: 2.867308 \n",
      "Epoch: 8, step: 1009, loss: 10.200102 \n",
      "Epoch: 8, step: 1010, loss: 10.756423 \n",
      "Epoch: 8, step: 1011, loss: 10.276297 \n",
      "Epoch: 8, step: 1012, loss: 5.491768 \n",
      "Epoch: 8, step: 1013, loss: 1.157551 \n",
      "Epoch: 8, step: 1014, loss: 22.715942 \n",
      "Epoch: 8, step: 1015, loss: 80.030418 \n",
      "Epoch: 8, step: 1016, loss: 31.566048 \n",
      "Epoch: 8, step: 1017, loss: 73.976593 \n",
      "Epoch: 8, step: 1018, loss: 37.653652 \n",
      "Epoch: 8, step: 1019, loss: 6.747612 \n",
      "Epoch: 8, step: 1020, loss: 33.714031 \n",
      "Epoch: 8, step: 1021, loss: 13.439537 \n",
      "Epoch: 8, step: 1022, loss: 0.001362 \n",
      "Epoch: 8, step: 1023, loss: 4.838083 \n",
      "Epoch: 8, step: 1024, loss: 4.512253 \n",
      "Epoch: 8, step: 1025, loss: 14.127893 \n",
      "Epoch: 8, step: 1026, loss: 10.283710 \n",
      "Epoch: 8, step: 1027, loss: 15.006848 \n",
      "Epoch: 8, step: 1028, loss: 24.113264 \n",
      "Epoch: 8, step: 1029, loss: 0.731213 \n",
      "Epoch: 8, step: 1030, loss: 39.066845 \n",
      "Epoch: 8, step: 1031, loss: 9.839044 \n",
      "Epoch: 8, step: 1032, loss: 4.480526 \n",
      "Epoch: 8, step: 1033, loss: 26.746567 \n",
      "Epoch: 8, step: 1034, loss: 25.627451 \n",
      "Epoch: 8, step: 1035, loss: 77.165443 \n",
      "Epoch: 8, step: 1036, loss: 35.321262 \n",
      "Epoch: 8, step: 1037, loss: 106.106483 \n",
      "Epoch: 8, step: 1038, loss: 7.822873 \n",
      "Epoch: 8, step: 1039, loss: 32.267620 \n",
      "Epoch: 8, step: 1040, loss: 5.082797 \n",
      "Epoch: 8, step: 1041, loss: 16.547474 \n",
      "Epoch: 8, step: 1042, loss: 2.821429 \n",
      "Epoch: 8, step: 1043, loss: 42.608231 \n",
      "Epoch: 8, step: 1044, loss: 47.386753 \n",
      "Epoch: 8, step: 1045, loss: 83.356262 \n",
      "Epoch: 8, step: 1046, loss: 20.280319 \n",
      "Epoch: 8, step: 1047, loss: 0.188931 \n",
      "Epoch: 8, step: 1048, loss: 66.286278 \n",
      "Epoch: 8, step: 1049, loss: 0.243754 \n",
      "Epoch: 8, step: 1050, loss: 7.000391 \n",
      "Epoch: 8, step: 1051, loss: 38.162727 \n",
      "Epoch: 8, step: 1052, loss: 48.110588 \n",
      "Epoch: 8, step: 1053, loss: 45.417110 \n",
      "Epoch: 8, step: 1054, loss: 0.048972 \n",
      "Epoch: 8, step: 1055, loss: 33.359638 \n",
      "Epoch: 8, step: 1056, loss: 11.649465 \n",
      "Epoch: 8, step: 1057, loss: 10.295961 \n",
      "Epoch: 8, step: 1058, loss: 1.897861 \n",
      "Epoch: 8, step: 1059, loss: 1.695431 \n",
      "Epoch: 8, step: 1060, loss: 10.364395 \n",
      "Epoch: 8, step: 1061, loss: 1.129426 \n",
      "Epoch: 8, step: 1062, loss: 39.194931 \n",
      "Epoch: 8, step: 1063, loss: 30.785435 \n",
      "Epoch: 8, step: 1064, loss: 40.036110 \n",
      "Epoch: 8, step: 1065, loss: 2.424497 \n",
      "Epoch: 8, step: 1066, loss: 0.833194 \n",
      "Epoch: 8, step: 1067, loss: 0.344304 \n",
      "Epoch: 8, step: 1068, loss: 1.259729 \n",
      "Epoch: 8, step: 1069, loss: 70.741226 \n",
      "Epoch: 8, step: 1070, loss: 12.009265 \n",
      "Epoch: 8, step: 1071, loss: 33.266220 \n",
      "Epoch: 8, step: 1072, loss: 17.015287 \n",
      "Epoch: 8, step: 1073, loss: 0.647952 \n",
      "Epoch: 8, step: 1074, loss: 22.351732 \n",
      "Epoch: 8, step: 1075, loss: 13.805378 \n",
      "Epoch: 8, step: 1076, loss: 77.224014 \n",
      "Epoch: 8, step: 1077, loss: 9.246068 \n",
      "Epoch: 8, step: 1078, loss: 24.625820 \n",
      "Epoch: 8, step: 1079, loss: 11.156969 \n",
      "Epoch: 8, step: 1080, loss: 1.226700 \n",
      "Epoch: 8, step: 1081, loss: 20.918663 \n",
      "Epoch: 8, step: 1082, loss: 23.671930 \n",
      "Epoch: 8, step: 1083, loss: 56.149654 \n",
      "Epoch: 8, step: 1084, loss: 19.304089 \n",
      "Epoch: 8, step: 1085, loss: 15.497914 \n",
      "Epoch: 8, step: 1086, loss: 5.184666 \n",
      "Epoch: 8, step: 1087, loss: 0.774117 \n",
      "Epoch: 8, step: 1088, loss: 1.475953 \n",
      "Epoch: 8, step: 1089, loss: 23.566662 \n",
      "Epoch: 8, step: 1090, loss: 63.557793 \n",
      "Epoch: 8, step: 1091, loss: 6.802512 \n",
      "Epoch: 8, step: 1092, loss: 58.429974 \n",
      "Epoch: 8, step: 1093, loss: 0.108622 \n",
      "Epoch: 8, step: 1094, loss: 38.002121 \n",
      "Epoch: 8, step: 1095, loss: 19.486488 \n",
      "Epoch: 8, step: 1096, loss: 5.708065 \n",
      "Epoch: 8, step: 1097, loss: 11.915895 \n",
      "Epoch: 8, step: 1098, loss: 31.621296 \n",
      "Epoch: 8, step: 1099, loss: 27.776279 \n",
      "Epoch: 8, step: 1100, loss: 0.994323 \n",
      "Epoch: 8, step: 1101, loss: 10.605894 \n",
      "Epoch: 8, step: 1102, loss: 0.113675 \n",
      "Epoch: 8, step: 1103, loss: 42.559402 \n",
      "Epoch: 8, step: 1104, loss: 52.598946 \n",
      "Epoch: 8, step: 1105, loss: 0.055498 \n",
      "Epoch: 8, step: 1106, loss: 14.313017 \n",
      "Epoch: 8, step: 1107, loss: 13.855809 \n",
      "Epoch: 8, step: 1108, loss: 2.342703 \n",
      "Epoch: 8, step: 1109, loss: 5.459129 \n",
      "Epoch: 8, step: 1110, loss: 4.814157 \n",
      "Epoch: 8, step: 1111, loss: 0.100626 \n",
      "Epoch: 8, step: 1112, loss: 22.929663 \n",
      "Epoch: 8, step: 1113, loss: 6.599267 \n",
      "Epoch: 8, step: 1114, loss: 9.263836 \n",
      "Epoch: 8, step: 1115, loss: 2.242157 \n",
      "Epoch: 8, step: 1116, loss: 14.065121 \n",
      "Epoch: 8, step: 1117, loss: 10.558248 \n",
      "Epoch: 8, step: 1118, loss: 18.405275 \n",
      "Epoch: 8, step: 1119, loss: 0.617911 \n",
      "Epoch: 8, step: 1120, loss: 0.127810 \n",
      "Epoch: 8, step: 1121, loss: 2.709063 \n",
      "Epoch: 8, step: 1122, loss: 2.553862 \n",
      "Epoch: 8, step: 1123, loss: 15.207559 \n",
      "Epoch: 8, step: 1124, loss: 40.431896 \n",
      "Epoch: 8, step: 1125, loss: 11.624389 \n",
      "Epoch: 8, step: 1126, loss: 44.138481 \n",
      "Epoch: 8, step: 1127, loss: 1.413070 \n",
      "Epoch: 8, step: 1128, loss: 17.496746 \n",
      "Epoch: 8, step: 1129, loss: 38.844093 \n",
      "Epoch: 8, step: 1130, loss: 14.640305 \n",
      "Epoch: 8, step: 1131, loss: 36.948936 \n",
      "Epoch: 8, step: 1132, loss: 23.134380 \n",
      "Epoch: 8, step: 1133, loss: 8.936183 \n",
      "Epoch: 9, step: 1134, loss: 1.892390 \n",
      "Epoch: 9, step: 1135, loss: 7.853556 \n",
      "Epoch: 9, step: 1136, loss: 8.930225 \n",
      "Epoch: 9, step: 1137, loss: 8.378730 \n",
      "Epoch: 9, step: 1138, loss: 2.520984 \n",
      "Epoch: 9, step: 1139, loss: 1.056571 \n",
      "Epoch: 9, step: 1140, loss: 13.813110 \n",
      "Epoch: 9, step: 1141, loss: 72.597275 \n",
      "Epoch: 9, step: 1142, loss: 26.536839 \n",
      "Epoch: 9, step: 1143, loss: 69.138901 \n",
      "Epoch: 9, step: 1144, loss: 34.041218 \n",
      "Epoch: 9, step: 1145, loss: 4.716687 \n",
      "Epoch: 9, step: 1146, loss: 30.614944 \n",
      "Epoch: 9, step: 1147, loss: 10.929650 \n",
      "Epoch: 9, step: 1148, loss: 0.001143 \n",
      "Epoch: 9, step: 1149, loss: 1.567812 \n",
      "Epoch: 9, step: 1150, loss: 3.080387 \n",
      "Epoch: 9, step: 1151, loss: 12.021276 \n",
      "Epoch: 9, step: 1152, loss: 9.004375 \n",
      "Epoch: 9, step: 1153, loss: 12.239528 \n",
      "Epoch: 9, step: 1154, loss: 18.128185 \n",
      "Epoch: 9, step: 1155, loss: 0.557498 \n",
      "Epoch: 9, step: 1156, loss: 36.825638 \n",
      "Epoch: 9, step: 1157, loss: 8.065542 \n",
      "Epoch: 9, step: 1158, loss: 3.240566 \n",
      "Epoch: 9, step: 1159, loss: 21.782398 \n",
      "Epoch: 9, step: 1160, loss: 21.045631 \n",
      "Epoch: 9, step: 1161, loss: 70.751007 \n",
      "Epoch: 9, step: 1162, loss: 28.667110 \n",
      "Epoch: 9, step: 1163, loss: 98.887512 \n",
      "Epoch: 9, step: 1164, loss: 7.289082 \n",
      "Epoch: 9, step: 1165, loss: 26.827282 \n",
      "Epoch: 9, step: 1166, loss: 4.209955 \n",
      "Epoch: 9, step: 1167, loss: 14.496973 \n",
      "Epoch: 9, step: 1168, loss: 2.161839 \n",
      "Epoch: 9, step: 1169, loss: 35.816414 \n",
      "Epoch: 9, step: 1170, loss: 41.587627 \n",
      "Epoch: 9, step: 1171, loss: 68.522049 \n",
      "Epoch: 9, step: 1172, loss: 16.874155 \n",
      "Epoch: 9, step: 1173, loss: 0.172840 \n",
      "Epoch: 9, step: 1174, loss: 59.583183 \n",
      "Epoch: 9, step: 1175, loss: 0.206831 \n",
      "Epoch: 9, step: 1176, loss: 5.543089 \n",
      "Epoch: 9, step: 1177, loss: 34.521168 \n",
      "Epoch: 9, step: 1178, loss: 44.393532 \n",
      "Epoch: 9, step: 1179, loss: 39.793560 \n",
      "Epoch: 9, step: 1180, loss: 0.041111 \n",
      "Epoch: 9, step: 1181, loss: 29.215969 \n",
      "Epoch: 9, step: 1182, loss: 9.452116 \n",
      "Epoch: 9, step: 1183, loss: 8.375965 \n",
      "Epoch: 9, step: 1184, loss: 1.316234 \n",
      "Epoch: 9, step: 1185, loss: 2.968273 \n",
      "Epoch: 9, step: 1186, loss: 9.122345 \n",
      "Epoch: 9, step: 1187, loss: 1.018842 \n",
      "Epoch: 9, step: 1188, loss: 35.535202 \n",
      "Epoch: 9, step: 1189, loss: 25.029022 \n",
      "Epoch: 9, step: 1190, loss: 35.887234 \n",
      "Epoch: 9, step: 1191, loss: 1.661283 \n",
      "Epoch: 9, step: 1192, loss: 0.710606 \n",
      "Epoch: 9, step: 1193, loss: 0.394964 \n",
      "Epoch: 9, step: 1194, loss: 0.686339 \n",
      "Epoch: 9, step: 1195, loss: 63.045502 \n",
      "Epoch: 9, step: 1196, loss: 6.317629 \n",
      "Epoch: 9, step: 1197, loss: 30.259401 \n",
      "Epoch: 9, step: 1198, loss: 10.828550 \n",
      "Epoch: 9, step: 1199, loss: 0.554261 \n",
      "Epoch: 9, step: 1200, loss: 19.471272 \n",
      "Epoch: 9, step: 1201, loss: 12.461458 \n",
      "Epoch: 9, step: 1202, loss: 66.991882 \n",
      "Epoch: 9, step: 1203, loss: 7.551315 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, step: 1204, loss: 19.631275 \n",
      "Epoch: 9, step: 1205, loss: 7.668934 \n",
      "Epoch: 9, step: 1206, loss: 0.757760 \n",
      "Epoch: 9, step: 1207, loss: 16.851528 \n",
      "Epoch: 9, step: 1208, loss: 19.557930 \n",
      "Epoch: 9, step: 1209, loss: 51.225204 \n",
      "Epoch: 9, step: 1210, loss: 14.975698 \n",
      "Epoch: 9, step: 1211, loss: 10.066834 \n",
      "Epoch: 9, step: 1212, loss: 3.978287 \n",
      "Epoch: 9, step: 1213, loss: 0.455304 \n",
      "Epoch: 9, step: 1214, loss: 1.240374 \n",
      "Epoch: 9, step: 1215, loss: 21.150238 \n",
      "Epoch: 9, step: 1216, loss: 56.264973 \n",
      "Epoch: 9, step: 1217, loss: 4.302561 \n",
      "Epoch: 9, step: 1218, loss: 50.575157 \n",
      "Epoch: 9, step: 1219, loss: 0.152793 \n",
      "Epoch: 9, step: 1220, loss: 35.614662 \n",
      "Epoch: 9, step: 1221, loss: 13.671635 \n",
      "Epoch: 9, step: 1222, loss: 3.279485 \n",
      "Epoch: 9, step: 1223, loss: 9.558020 \n",
      "Epoch: 9, step: 1224, loss: 29.168476 \n",
      "Epoch: 9, step: 1225, loss: 23.548368 \n",
      "Epoch: 9, step: 1226, loss: 0.813265 \n",
      "Epoch: 9, step: 1227, loss: 7.514618 \n",
      "Epoch: 9, step: 1228, loss: 0.072373 \n",
      "Epoch: 9, step: 1229, loss: 36.256355 \n",
      "Epoch: 9, step: 1230, loss: 46.817928 \n",
      "Epoch: 9, step: 1231, loss: 0.087446 \n",
      "Epoch: 9, step: 1232, loss: 13.514969 \n",
      "Epoch: 9, step: 1233, loss: 11.541360 \n",
      "Epoch: 9, step: 1234, loss: 1.076361 \n",
      "Epoch: 9, step: 1235, loss: 4.284693 \n",
      "Epoch: 9, step: 1236, loss: 3.195752 \n",
      "Epoch: 9, step: 1237, loss: 0.065519 \n",
      "Epoch: 9, step: 1238, loss: 21.832525 \n",
      "Epoch: 9, step: 1239, loss: 5.862287 \n",
      "Epoch: 9, step: 1240, loss: 8.348484 \n",
      "Epoch: 9, step: 1241, loss: 1.776264 \n",
      "Epoch: 9, step: 1242, loss: 11.373014 \n",
      "Epoch: 9, step: 1243, loss: 8.629286 \n",
      "Epoch: 9, step: 1244, loss: 15.521469 \n",
      "Epoch: 9, step: 1245, loss: 0.474027 \n",
      "Epoch: 9, step: 1246, loss: 0.109901 \n",
      "Epoch: 9, step: 1247, loss: 1.742520 \n",
      "Epoch: 9, step: 1248, loss: 2.533905 \n",
      "Epoch: 9, step: 1249, loss: 14.011557 \n",
      "Epoch: 9, step: 1250, loss: 35.207504 \n",
      "Epoch: 9, step: 1251, loss: 10.934399 \n",
      "Epoch: 9, step: 1252, loss: 39.143749 \n",
      "Epoch: 9, step: 1253, loss: 1.219543 \n",
      "Epoch: 9, step: 1254, loss: 13.698680 \n",
      "Epoch: 9, step: 1255, loss: 35.373596 \n",
      "Epoch: 9, step: 1256, loss: 12.678135 \n",
      "Epoch: 9, step: 1257, loss: 31.186596 \n",
      "Epoch: 9, step: 1258, loss: 18.731205 \n",
      "Epoch: 9, step: 1259, loss: 8.781786 \n"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "N = train_X.shape[0]\n",
    "\n",
    "#bắt đầu một session\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        pos = 0\n",
    "\n",
    "        while pos < N:\n",
    "            batch_X = train_X[pos:pos+batch_size]\n",
    "            batch_Y = train_Y[pos:pos+batch_size]\n",
    "            feed_dict = {x: batch_X,y: batch_Y,keep_prob: dropout_prob}\n",
    "            _,summary,loss = sess.run([train_op,merged,l], feed_dict=feed_dict)\n",
    "            print(\"Epoch: %d, step: %d, loss: %f \"% (epoch,step,loss))\n",
    "            train_writer.add_summary(summary,step)\n",
    "\n",
    "            step += 1\n",
    "            pos += batch_size\n",
    "\n",
    "    #dự đoán\n",
    "    train_y_pred = sess.run(y_pred, feed_dict={x:train_X,keep_prob:1.0})\n",
    "    valid_y_pred = sess.run(y_pred, feed_dict={x:valid_X,keep_prob:1.0})\n",
    "    test_y_pred = sess.run(y_pred, feed_dict={x:test_X,keep_prob:1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Weighted Classification Accuracy: 0.777218\n",
      "Valid Weighted Classification Accuracy: 0.644231\n",
      "Test Weighted Classification Accuracy: 0.684198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "#đưa ra độ chính xác của dự đoán\n",
    "train_weighted_score = accuracy_score(train_Y, train_y_pred, sample_weight=train_w)\n",
    "print(\"Train Weighted Classification Accuracy: %f\" % train_weighted_score)\n",
    "valid_weighted_score = accuracy_score(valid_Y, valid_y_pred, sample_weight=valid_w)\n",
    "print(\"Valid Weighted Classification Accuracy: %f\" % valid_weighted_score)\n",
    "test_weighted_score = accuracy_score(test_Y, test_y_pred, sample_weight=test_w)\n",
    "print(\"Test Weighted Classification Accuracy: %f\" % test_weighted_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my-rekit-env)",
   "language": "python",
   "name": "my-rekit-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
